
理念：方便模型调参测试，尽量做到自动化，减少人为的参数设置和出错的可能性，省去每次训练模型都要设置参数的麻烦；尝试不同的模型的时候只需要去改动定义的代码；
其余分析全部自动生成。

模型测试调参（手动，grid_search)
    模型定义在根目录，包括数据处理，特征选取，模型定义，模型参数，模型描述；每次运行测试自动保存
    生成 ID，后续有需要可根据 id 来 获取模型 (考虑用文件 hash 的方式生成 id)
    （两个定义文件可以直接定义在 ipynb 里面？）

汇总模型得分(取消)
    需要在根目录定义数据集划分方法和 evaluation metric
    一张表上列出所有模型的得分，同一张表上的模型有相同的数据集划分和evaluation metric
    多次划分训练集测试集来测试，保存每一次测试结果，汇总模型得分，计算得分可靠性